[
  {
    "model_type": "MoE",
    "model_name": "DeepSeek-R1-0528",
    "parameters": {
      "model_total_params": 68500000000,
      "expert_params_per_expert": 2620000000,
      "shared_params": 14400000000,
      "vocab_size": 129280,
      "hidden_size": 7168,
      "num_hidden_layers": 61,
      "num_attention_heads": 128,
      "num_key_value_heads": 128,
      "intermediate_size": 18432,
      "num_local_experts": 256,
      "num_experts_per_tok": 8,
      "max_position_embeddings": 163840,
      "input_length": 163840,
      "output_length": 163840
    }
  },
  {
    "model_type": "Dense",
    "model_name": "DeepSeek-R1-Distill-Llama-70B",
    "parameters": {
      "model_total_params": 70000000000,
      "vocab_size": 128256,
      "hidden_size": 8192,
      "num_hidden_layers": 80,
      "num_attention_heads": 64,
      "num_key_value_heads": 8,
      "max_position_embeddings": 131072
    }
  },
  {
    "model_type": "Dense",
    "model_name": "DeepSeek-R1-Distill-Qwen-32B",
    "parameters": {
      "model_total_params": 32000000000,
      "vocab_size": 152064,
      "hidden_size": 5120,
      "num_hidden_layers": 64,
      "num_attention_heads": 40,
      "num_key_value_heads": 8,
      "max_position_embeddings": 131072
    }
  },
  {
    "model_type": "Dense",
    "model_name": "DeepSeek-R1-Distill-Qwen-14B",
    "parameters": {
      "model_total_params": 14000000000,
      "vocab_size": 152064,
      "hidden_size": 5120,
      "num_hidden_layers": 48,
      "num_attention_heads": 40,
      "num_key_value_heads": 8,
      "max_position_embeddings": 131072
    }
  },
  {
    "model_type": "Dense",
    "model_name": "DeepSeek-R1-Distill-Llama-8B ",
    "parameters": {
      "model_total_params": 8000000000,
      "vocab_size": 128256,
      "hidden_size": 4096,
      "num_hidden_layers": 32,
      "num_attention_heads": 32,
      "num_key_value_heads": 8,
      "max_position_embeddings": 8192
    }
  },
  {
    "model_type": "Dense",
    "model_name": "DeepSeek-R1-Distill-Qwen-7B",
    "parameters": {
      "model_total_params": 7000000000,
      "vocab_size": 152064,
      "hidden_size": 3584,
      "num_hidden_layers": 28,
      "num_attention_heads": 28,
      "num_key_value_heads": 4,
      "max_position_embeddings": 131072
    }
  },
  {
    "model_type": "Dense",
    "model_name": "DeepSeek-R1-Distill-Qwen-1.5B",
    "parameters": {
      "model_total_params": 1500000000,
      "vocab_size": 151936,
      "hidden_size": 1536,
      "num_hidden_layers": 28,
      "num_attention_heads": 12,
      "num_key_value_heads": 2,
      "max_position_embeddings": 131072
    }
  },
  {
    "model_type": "Dense",
    "model_name": "DeepSeek-R1-0528-Qwen3-8B",
    "parameters": {
      "model_total_params": 8000000000,
      "vocab_size": 151936,
      "hidden_size": 4096,
      "num_hidden_layers": 36,
      "num_attention_heads": 32,
      "num_key_value_heads": 8,
      "max_position_embeddings": 131072
    }
  },
  {
    "model_type": "MoE",
    "model_name": "Llama-4-Scout-17B-16E",
    "parameters": {
      "model_total_params": 109000000000,
      "expert_params_per_expert": 6440000000,
      "shared_params": 6000000000,
      "vocab_size": 202048,
      "hidden_size": 5120,
      "num_hidden_layers": 48,
      "num_attention_heads": 40,
      "num_key_value_heads": 8,
      "intermediate_size": 16384,
      "num_local_experts": 16,
      "num_experts_per_tok": 1,
      "max_position_embeddings": 10485760,
      "input_length": 8192,
      "output_length": 2048
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Llama-3.3-70B-Instruct",
    "parameters": {
      "model_total_params": 70553706496,
      "vocab_size": 128256,
      "hidden_size": 8192,
      "num_hidden_layers": 80,
      "num_attention_heads": 64,
      "num_key_value_heads": 8,
      "max_position_embeddings": 131072
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Llama-3.2-3B-Instruct",
    "parameters": {
      "model_total_params": 3000000000,
      "vocab_size": 128256,
      "hidden_size": 3072,
      "num_hidden_layers": 28,
      "num_attention_heads": 24,
      "num_key_value_heads": 8,
      "max_position_embeddings": 131072
    }
  },
  {
    "model_type": "Multimodal",
    "model_name": "Llama-3.2-11B-Vision-Instruct",
    "parameters": {
      "llm_base_params": 11000000000,
      "vision_params": 632000000,
      "audio_params": 0,
      "has_vision_modal": true,
      "has_audio_modal": false,
      "vocab_size": 128256,
      "hidden_size": 4096,
      "num_hidden_layers": 40,
      "num_attention_heads": 32,
      "num_key_value_heads": 8,
      "max_position_embeddings": 131072,
      "image_input_size": 560,
      "patch_size": 14,
      "audio_input_length": 0,
      "audio_sample_rate": 0
    }
  },
  {
    "model_type": "Multimodal",
    "model_name": "Llama-3.2-90B-Vision-Instruct",
    "parameters": {
      "llm_base_params": 90000000000,
      "vision_params": 639728640,
      "audio_params": 0,
      "has_vision_modal": true,
      "has_audio_modal": false,
      "vocab_size": 128256,
      "hidden_size": 8192,
      "num_hidden_layers": 100,
      "num_attention_heads": 64,
      "num_key_value_heads": 8,
      "max_position_embeddings": 131072,
      "image_input_size": 560,
      "patch_size": 14,
      "audio_input_length": 0,
      "audio_sample_rate": 0
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Llama-3.1-8B-Instruct",
    "parameters": {
      "model_total_params": 8000000000,
      "vocab_size": 128256,
      "hidden_size": 4096,
      "num_hidden_layers": 32,
      "num_attention_heads": 32,
      "num_key_value_heads": 8,
      "max_position_embeddings": 131072
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Llama-3.1-70B-Instruct",
    "parameters": {
      "model_total_params": 70000000000,
      "vocab_size": 128256,
      "hidden_size": 8192,
      "num_hidden_layers": 80,
      "num_attention_heads": 64,
      "num_key_value_heads": 8,
      "max_position_embeddings": 131072
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Llama-3.1-405B-Instruct",
    "parameters": {
      "model_total_params": 405000000000,
      "vocab_size": 128256,
      "hidden_size": 8192,
      "num_hidden_layers": 80,
      "num_attention_heads": 64,
      "num_key_value_heads": 8,
      "max_position_embeddings": 131072
    }
  },
  {
    "model_type": "MoE",
    "model_name": "Qwen3-235B-A22B",
    "parameters": {
      "model_total_params": 235000000000,
      "expert_params_per_expert": 1775000000,
      "shared_params": 7800000000,
      "vocab_size": 151936,
      "hidden_size": 4096,
      "num_hidden_layers": 94,
      "num_attention_heads": 64,
      "num_key_value_heads": 4,
      "intermediate_size": 12288,
      "num_local_experts": 128,
      "num_experts_per_tok": 8,
      "max_position_embeddings": 40960
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Qwen3-32B",
    "parameters": {
      "model_total_params": 32000000000,
      "vocab_size": 151936,
      "hidden_size": 5120,
      "num_hidden_layers": 64,
      "num_attention_heads": 64,
      "num_key_value_heads": 8,
      "max_position_embeddings": 40960
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Qwen3-14B",
    "parameters": {
      "model_total_params": 14000000000,
      "vocab_size": 151936,
      "hidden_size": 5120,
      "num_hidden_layers": 40,
      "num_attention_heads": 40,
      "num_key_value_heads": 8,
      "max_position_embeddings": 40960
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Qwen3-8B",
    "parameters": {
      "model_total_params": 8000000000,
      "vocab_size": 151936,
      "hidden_size": 4096,
      "num_hidden_layers": 36,
      "num_attention_heads": 32,
      "num_key_value_heads": 8,
      "max_position_embeddings": 40960
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Qwen3-4B",
    "parameters": {
      "model_total_params": 4020255840,
      "vocab_size": 151936,
      "hidden_size": 2560,
      "num_hidden_layers": 36,
      "num_attention_heads": 32,
      "num_key_value_heads": 8,
      "max_position_embeddings": 40960
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Llama3-8B",
    "parameters": {
      "model_total_params": 8000000000,
      "vocab_size": 128256,
      "hidden_size": 4096,
      "num_hidden_layers": 32,
      "num_attention_heads": 32,
      "num_key_value_heads": 8,
      "max_position_embeddings": 8192
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Llama3-70B",
    "parameters": {
      "model_total_params": 70000000000,
      "vocab_size": 128256,
      "hidden_size": 8192,
      "num_hidden_layers": 80,
      "num_attention_heads": 64,
      "num_key_value_heads": 8,
      "max_position_embeddings": 8192
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Llama2-7B",
    "parameters": {
      "model_total_params": 7000000000,
      "vocab_size": 32000,
      "hidden_size": 4096,
      "num_hidden_layers": 32,
      "num_attention_heads": 32,
      "num_key_value_heads": 32,
      "max_position_embeddings": 4096
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Llama2-13B",
    "parameters": {
      "model_total_params": 13000000000,
      "vocab_size": 32000,
      "hidden_size": 5120,
      "num_hidden_layers": 40,
      "num_attention_heads": 40,
      "num_key_value_heads": 40,
      "max_position_embeddings": 4096
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Llama2-70B",
    "parameters": {
      "model_total_params": 70000000000,
      "vocab_size": 32000,
      "hidden_size": 8192,
      "num_hidden_layers": 80,
      "num_attention_heads": 64,
      "num_key_value_heads": 8,
      "max_position_embeddings": 4096
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Baichuan2-13B",
    "parameters": {
      "model_total_params": 13000000000,
      "vocab_size": 125696,
      "hidden_size": 5120,
      "num_hidden_layers": 40,
      "num_attention_heads": 40,
      "num_key_value_heads": 40,
      "max_position_embeddings": 4096
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Qwen2-7B",
    "parameters": {
      "model_total_params": 7000000000,
      "vocab_size": 152064,
      "hidden_size": 3584,
      "num_hidden_layers": 28,
      "num_attention_heads": 28,
      "num_key_value_heads": 4,
      "max_position_embeddings": 32768
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Qwen2-72B",
    "parameters": {
      "model_total_params": 72000000000,
      "vocab_size": 152064,
      "hidden_size": 8192,
      "num_hidden_layers": 80,
      "num_attention_heads": 64,
      "num_key_value_heads": 8,
      "max_position_embeddings": 32768
    }
  },
  {
    "model_type": "Dense",
    "model_name": "Yi-34B",
    "parameters": {
      "model_total_params": 34000000000,
      "vocab_size": 64000,
      "hidden_size": 7168,
      "num_hidden_layers": 60,
      "num_attention_heads": 56,
      "num_key_value_heads": 8,
      "max_position_embeddings": 4096
    }
  },
  {
    "model_type": "MoE",
    "model_name": "Mixtral-8x7B",
    "parameters": {
      "model_total_params": 46700000000,
      "expert_params_per_expert": 5500000000,
      "shared_params": 2700000000,
      "vocab_size": 32000,
      "hidden_size": 4096,
      "num_hidden_layers": 32,
      "num_attention_heads": 32,
      "num_key_value_heads": 8,
      "intermediate_size": 14336,
      "num_local_experts": 8,
      "num_experts_per_tok": 2,
      "max_position_embeddings": 32768,
      "input_length": 2048,
      "output_length": 2048
    }
  },
  {
    "model_type": "MoE",
    "model_name": "Mixtral-8x22B",
    "parameters": {
      "model_total_params": 141700000000,
      "expert_params_per_expert": 16000000000,
      "shared_params": 13700000000,
      "vocab_size": 32768,
      "hidden_size": 6144,
      "num_hidden_layers": 56,
      "num_attention_heads": 48,
      "num_key_value_heads": 8,
      "intermediate_size": 16384,
      "num_local_experts": 8,
      "num_experts_per_tok": 2,
      "max_position_embeddings": 65536,
      "input_length": 2048,
      "output_length": 2048
    }
  },
  {
    "model_type": "MoE",
    "model_name": "Qwen1.5-MoE-A2.7B",
    "parameters": {
      "model_total_params": 14300000000,
      "expert_params_per_expert": 210000000,
      "shared_params": 896000000,
      "vocab_size": 151936,
      "hidden_size": 2048,
      "num_hidden_layers": 24,
      "num_attention_heads": 16,
      "num_key_value_heads": 16,
      "intermediate_size": 5632,
      "num_local_experts": 64,
      "num_experts_per_tok": 4,
      "max_position_embeddings": 32768,
      "input_length": 2048,
      "output_length": 2048
    }
  },
  {
    "model_type": "Multimodal",
    "model_name": "llava-1.5-7b",
    "parameters": {
      "llm_base_params": 6738415616,
      "vision_params": 309006848,
      "audio_params": 0,
      "has_vision_modal": true,
      "has_audio_modal": false,
      "vocab_size": 32000,
      "hidden_size": 4096,
      "num_hidden_layers": 32,
      "num_attention_heads": 32,
      "num_key_value_heads": 32,
      "max_position_embeddings": 4096,
      "image_input_size": 336,
      "patch_size": 14,
      "audio_input_length": 30,
      "audio_sample_rate": 16000
    }
  },
  {
    "model_type": "Multimodal",
    "model_name": "llava-1.5-13b",
    "parameters": {
      "llm_base_params": 13000000000,
      "vision_params": 309006848,
      "has_vision_modal": true,
      "has_audio_modal": false,
      "vocab_size": 32064,
      "hidden_size": 5120,
      "num_hidden_layers": 40,
      "num_attention_heads": 40,
      "num_key_value_heads": 40,
      "max_position_embeddings": 4096,
      "image_input_size": 336,
      "patch_size": 14
    }
  },
  {
    "model_type": "Multimodal",
    "model_name": "Qwen2-VL-72B",
    "parameters": {
      "llm_base_params": 7000000000,
      "vision_params": 600000000,
      "audio_params": 0,
      "has_vision_modal": true,
      "has_audio_modal": false,
      "vocab_size": 152064,
      "hidden_size": 8192,
      "num_hidden_layers": 80,
      "num_attention_heads": 64,
      "num_key_value_heads": 8,
      "max_position_embeddings": 32768,
      "image_input_size": 448,
      "patch_size": 14,
      "audio_input_length": 0,
      "audio_sample_rate": 0
    }
  },
  {
    "model_type": "Multimodal",
    "model_name": "Yi-VL-34B",
    "parameters": {
      "llm_base_params": 34000000000,
      "vision_params": 600000000,
      "audio_params": 0,
      "has_vision_modal": true,
      "has_audio_modal": false,
      "vocab_size": 64000,
      "hidden_size": 7168,
      "num_hidden_layers": 60,
      "num_attention_heads": 56,
      "num_key_value_heads": 8,
      "max_position_embeddings": 4096,
      "image_input_size": 448,
      "patch_size": 14,
      "audio_input_length": 0,
      "audio_sample_rate": 0
    }
  },
  {
    "model_type": "MoE",
    "model_name": "盘古 Pro MoE (72B-A16B)",
    "parameters": {
      "model_total_params": 72000000000,
      "expert_params_per_expert": 659850240,
      "shared_params": 29769584640,
      "vocab_size": 153376,
      "hidden_size": 5120,
      "num_hidden_layers": 48,
      "num_attention_heads": 40,
      "num_key_value_heads": 8,
      "intermediate_size": 1344,
      "num_local_experts": 64,
      "num_experts_per_tok": 8,
      "max_position_embeddings": 131072,
      "input_length": null,
      "output_length": null
    }
  }
]